{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import usefull_methods as um\n",
    "import do_actions as do\n",
    "\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_shape(data, name):\n",
    "    print(\"{0}: {1}\".format(name, data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_missing(data):\n",
    "    # df.isnull() Return a boolean same-sized object indicating if the values are null.\n",
    "    # df.any() Return whether any element is True over requested axis\n",
    "    #\n",
    "    missing = data.columns[data.isnull().any()].tolist()\n",
    "    return missing\n",
    "\n",
    "# Looking at categorical values\n",
    "def cat_exploration(data, column):\n",
    "    return data[column].value_counts()\n",
    "\n",
    "# deleting missing data\n",
    "def handle_missing(data, column, value):\n",
    "    data.loc[data[column].isnull(),column] = value\n",
    "    return data\n",
    "\n",
    "def count_missing(data):\n",
    "    data[show_missing(data)].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_handle_missing_data(data):\n",
    "    data = handle_missing(data, 'PoolQC', 'None')\n",
    "    data = handle_missing(data, 'MiscFeature', 'None' )\n",
    "    data = handle_missing(data, 'Alley', 'None' )\n",
    "    data = handle_missing(data, 'Fence', 'None' )\n",
    "    data = handle_missing(data, 'FireplaceQu', 'None' )\n",
    "    data = handle_missing(data, 'LotFrontage', 0.0 )\n",
    "    \n",
    "    data = handle_missing(data, 'GarageType', 'None' )\n",
    "    data = handle_missing(data, 'GarageYrBlt', 0.0 )\n",
    "    data = handle_missing(data, 'GarageFinish', 'None' )\n",
    "    data = handle_missing(data, 'GarageQual', 'None' )\n",
    "    data = handle_missing(data, 'GarageCond', 'None' )\n",
    "    data = handle_missing(data, 'GarageArea', 0.0 )\n",
    "    data = handle_missing(data, 'GarageCars', 'None' )\n",
    " \n",
    "    data = handle_missing(data, 'BsmtQual', 'None' )\n",
    "    data = handle_missing(data, 'BsmtCond', 'None' )\n",
    "    data = handle_missing(data, 'BsmtExposure', 'None' )\n",
    "    data = handle_missing(data, 'BsmtFinType1', 'None' )\n",
    "    data = handle_missing(data, 'BsmtFinType2', 'None' )\n",
    "    \n",
    "    data = handle_missing(data, 'BsmtFinSF1', 0.0)\n",
    "    data = handle_missing(data, 'BsmtFinSF2', 0.0 )\n",
    "    data = handle_missing(data, 'BsmtFullBath', 'None' )\n",
    "    data = handle_missing(data, 'BsmtHalfBath', 'None' )\n",
    "    data = handle_missing(data, 'BsmtUnfSF', 0.0 )\n",
    "    \n",
    "    data = handle_missing(data, 'TotalBsmtSF', 0.0 )\n",
    "    data = handle_missing(data, 'MasVnrArea', 0.0 )\n",
    "    \n",
    "    # missing data replaced by most frequent value\n",
    "    data = handle_missing(data, 'MasVnrType', 'None' )\n",
    "    data = handle_missing(data, 'Electrical','SBrkr' )\n",
    "    data = handle_missing(data, 'Functional','Typ' )\n",
    "    data = handle_missing(data, 'KitchenQual','TA' )\n",
    "    data = handle_missing(data, 'MSZoning','RL' )\n",
    "    data = handle_missing(data, 'Exterior1st','VinylSd' )\n",
    "    data = handle_missing(data, 'Exterior2nd','VinylSd' )    \n",
    "    data = handle_missing(data, 'SaleType','WD' )\n",
    "    \n",
    "    # not in training set\n",
    "    data = data.drop(['Utilities'], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Categorical  Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricaldata = ('MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'PoolQC',\n",
    "                       'Street', 'BsmtCond', 'GarageCond', 'GarageQual',\n",
    "                      'BsmtQual', 'CentralAir', 'ExterQual', 'ExterCond', 'HeatingQC',\n",
    "                      'KitchenQual', 'BsmtFinType1','BsmtFinType2', 'Functional',\n",
    "                      'BsmtExposure', 'GarageFinish','LandSlope', 'LotShape', 'MSZoning',\n",
    "                      'LandContour', 'LotConfig', 'Neighborhood', 'Condition1',\n",
    "                      'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "                      'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', 'GarageType',\n",
    "                      'PavedDrive', 'SaleType', 'SaleCondition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Skew #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "\n",
    "def check_skewed(data, treshold):\n",
    "    skewcheck = data.dtypes[data.dtypes != \"object\"].index\n",
    "    skewed = data[skewcheck].skew().sort_values(ascending=False)\n",
    "    skew_df = pd.DataFrame(skewed)\n",
    "    skew_df = abs(skew_df)\n",
    "    skew_df = skew_df[skew_df > treshold]\n",
    "    skew_df = skew_df.dropna()\n",
    "    return skew_df\n",
    "\n",
    "def unskew_boxcox1p(data, skew_df, lm):\n",
    "    needs_fixing = skew_df.index\n",
    "    for x in needs_fixing:\n",
    "        data[x] = boxcox1p(data[x], lm)\n",
    "        data[x] += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Drop #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_remove_outliers_simple(data):\n",
    "    data = data.drop(data[(data['GrLivArea']>4000) & (data['SalePrice']<300000)].index)\n",
    "    return data\n",
    "\n",
    "def do_outliers_Ids(data):\n",
    "    a = data['GrLivArea']>4000\n",
    "    b = data['SalePrice']<300000\n",
    "    f = [a, b]\n",
    "    return pd.concat(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train: (1460, 81)\n",
      "data_test: (1459, 80)\n",
      "train_ids: (1458L,)\n",
      "test_ids: (1459L,)\n",
      "all_data: (2917, 298)\n",
      "features_train: (1458, 298)\n",
      "features_test: (1459, 298)\n",
      "features_train: (1458, 297)\n",
      "features_test: (1459, 297)\n",
      "target_train: (1458L,)\n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "stopwatch.start()\n",
    "data_train, data_test = um.load_data()\n",
    "\n",
    "print_shape( data_train, 'data_train')\n",
    "print_shape( data_test, 'data_test')\n",
    "\n",
    "# get outliers Id, will be removed later\n",
    "data_train = do.remove_this_Ids(data_train, [524, 1299])\n",
    "\n",
    "# target variable, preserve and remove from data\n",
    "target_train = data_train[['SalePrice','Id']]\n",
    "data_train = data_train.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "# concatenate test and train data\n",
    "train_ids = data_train['Id']\n",
    "test_ids = data_test['Id']\n",
    "\n",
    "print_shape( train_ids, 'train_ids')\n",
    "print_shape( test_ids, 'test_ids')\n",
    "\n",
    "all_data = pd.concat([data_train, data_test])\n",
    "all_ids = all_data['Id']\n",
    "\n",
    "all_data = do_handle_missing_data(all_data)\n",
    "all_data = do.add_TotalSF(all_data)\n",
    "\n",
    "dropcolumns = ('BsmtFinSF2','BsmtHalfBath','MiscVal','LowQualFinSF', 'YrSold', 'OverallCond', 'MSSubClass', 'EnclosedPorch',\n",
    " 'KitchenAbvGr',   'BsmtFullBath', 'BsmtUnfSF', 'BedroomAbvGr', 'ScreenPorch',  'MoSold', '3SsnPorch')\n",
    "\n",
    "# LABEL ENCODER\n",
    "all_data = do.encode_categorical(all_data, categoricaldata)\n",
    "\n",
    "# DROP SOME DATA\n",
    "drop_columns = False\n",
    "do_dummies = False\n",
    "if drop_columns:\n",
    "    all_data = all_data.drop(list(dropcolumns), axis = 1)\n",
    "    to_dummies = set(categoricaldata) - set(dropcolumns)\n",
    "    if do_dummies:\n",
    "        all_data = pd.get_dummies(all_data, columns=list(to_dummies))\n",
    "else:\n",
    "    if do_dummies:\n",
    "        all_data = pd.get_dummies(all_data, columns=list(categoricaldata))\n",
    "\n",
    "keep_cols = all_data.select_dtypes(include = ['number']).columns\n",
    "all_data = all_data[keep_cols]\n",
    "\n",
    "# UNSKEW\n",
    "unskew_data = False\n",
    "if unskew_data:\n",
    "    skewed_data = check_skewed(all_data, 0.75)\n",
    "    all_data = unskew_boxcox1p(all_data, skewed_data, 0.25)\n",
    "\n",
    "print_shape( all_data, 'all_data')\n",
    "\n",
    "all_data['Id'] = all_ids\n",
    "\n",
    "features_train = all_data[all_data['Id'].isin(train_ids)]\n",
    "features_test = all_data[all_data['Id'].isin(test_ids)]\n",
    "print_shape( features_train, 'features_train')\n",
    "print_shape( features_test, 'features_test')\n",
    "\n",
    "#features_train = features_train.drop(features_train['Id'].isin(outliers_ids))\n",
    "#target_train = target_train.drop(target_train['Id'].isin(outliers_ids))\n",
    "\n",
    "features_train = features_train.drop(['Id'], axis = 1)\n",
    "features_test = features_test.drop(['Id'], axis = 1)\n",
    "target_train = target_train.drop(['Id'], axis = 1)\n",
    "target_train = target_train['SalePrice'].values\n",
    "\n",
    "print_shape( features_train, 'features_train')\n",
    "print_shape( features_test, 'features_test')\n",
    "print_shape( target_train, 'target_train')\n",
    "\n",
    "print(features_train.columns[features_train.isnull().any()])\n",
    "print(features_test.columns[features_test.isnull().any()])\n",
    "\n",
    "features_test.head()\n",
    "\n",
    "features = features_train\n",
    "target = target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def RMSE(y, y_pred):\n",
    "    return np.sqrt(np.sum(np.square(y_pred-y))/len(y))\n",
    "\n",
    "def RMSElog(y, y_pred):\n",
    "    y = np.log(y)\n",
    "    y_pred = np.log(y_pred)\n",
    "    return np.sqrt(np.sum(np.square(y_pred-y))/len(y))\n",
    "\n",
    "def calc_metrics(y, y_pred):\n",
    "    r2score = r2_score(y, y_pred)\n",
    "    rmse_log = RMSElog(y, y_pred)\n",
    "    rmse = RMSE(y, y_pred)\n",
    "    return rmse, rmse_log,  r2score\n",
    "\n",
    "def print_metrics (rmse, rmse_log,r2score):\n",
    "    print('RMSE               : {:.4f}'.format(rmse, prec=5))\n",
    "    print('RMSE of logarithms : {:.4f}'.format(rmse_log, prec=5))\n",
    "    print('R2 score           : {:.6f}'.format(r2score, prec = 5))\n",
    "    \n",
    "def show_metrics(y, y_pred):\n",
    "    rmse, rmse_log, r2score = calc_metrics(y, y_pred)\n",
    "    print_metrics(rmse, rmse_log, r2score )\n",
    "    return rmse, rmse_log, r2score\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(colsample_bytree=0.2, learning_rate=0.05, max_depth=2, n_estimators=1700, random_state = 133)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=1000, max_features = 'sqrt',  learning_rate = 0.05, random_state=1234)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(max_depth=20, n_estimators=375, max_features = 'sqrt', oob_score=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class GridSearchObject:\n",
    "    def __init__(self, estimator, grid_params):\n",
    "        self.estimator = estimator\n",
    "        self.name = type(estimator).__name__\n",
    "        self.grid_params = grid_params\n",
    "\n",
    "class GridSearchBatch:\n",
    "    def __init__(self, features,target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.results_list = []\n",
    "        self.batch_list = []\n",
    "        self.stopwatch = Stopwatch()\n",
    "\n",
    "    def add(self, grid_search_object):\n",
    "        self.batch_list.append(grid_search_object)\n",
    "        \n",
    "    def run(self):\n",
    "        scorer = make_scorer(r2_score)\n",
    "        for gso in self.batch_list:\n",
    "            self.stopwatch.start()\n",
    "            grid_search = GridSearchCV(gso.estimator, gso.grid_params, scoring = scorer, cv=5, n_jobs =-1)\n",
    "            grid_search.fit(self.features, self.target)\n",
    "            duration = self.stopwatch.elapsed_time()\n",
    "            cv_results = grid_search.cv_results_\n",
    "            cv_results.update({'duration':duration})\n",
    "            cv_results.update({'name':gso.name})\n",
    "            cv_results.update({'grid_params':gso.grid_params})\n",
    "            self.results_list.append(cv_results)\n",
    "            self.stopwatch.print_elapsed_time()\n",
    "            print(gso.name)\n",
    "            print(grid_search.best_estimator_)\n",
    "            print(grid_search.best_score_ )\n",
    "        return self.results_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = GridSearchBatch(features, target)\n",
    "\n",
    "# param_grid - dict or list of dictionaries\n",
    "param_grid = {'max_depth': [ 20, 25, 30, 32  ], 'n_estimators':[375, 400, 500, 600, 700 ]}\n",
    "#param_grid = {'max_depth': [ 15, 20, 25, 30, 32  ], 'n_estimators':[275, 300, 325, 350, 375, 400 ]}\n",
    "#param_grid = {'max_depth': [ 30, 32, 36, ], 'n_estimators':[275, 300, 325, ]}\n",
    "#cv_scores = cross_val_score(rf_test, features, target, cv = 5, n_jobs = -1)\n",
    "estimator = RandomForestRegressor(max_features = 100, oob_score=True, random_state=458)\n",
    "#batch.add( GridSearchObject(estimator, param_grid))\n",
    "\n",
    "param_grid = {'max_depth': [ 2, 3, 5 ], 'n_estimators':[ 800, 900, 1000, 1100, 1200  ]}\n",
    "estimator = GradientBoostingRegressor( max_features = 100,  random_state=1234)\n",
    "#batch.add( GridSearchObject(estimator, param_grid))\n",
    "\n",
    "param_grid = {'max_depth': [ 2, 3, 4], 'n_estimators':[ 1600, 1700, 1800, 2000 ]}\n",
    "estimator = XGBRegressor(colsample_bytree=0.2, learning_rate=0.05, random_state = 1337 )\n",
    "batch.add( GridSearchObject(estimator, param_grid))\n",
    "\n",
    "# takes long !!!\n",
    "# results = batch.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elapsed time: 00:06:47\n",
    "\n",
    "RandomForestRegressor\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
    "           max_features=100, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
    "           oob_score=True, random_state=458, verbose=0, warm_start=False)\n",
    "0.898382838789\n",
    "\n",
    "Elapsed time: 00:06:54\n",
    "\n",
    "GradientBoostingRegressor\n",
    "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='ls', max_depth=2, max_features=100,\n",
    "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "             min_samples_leaf=1, min_samples_split=2,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "             presort='auto', random_state=1234, subsample=1.0, verbose=0,\n",
    "             warm_start=False)\n",
    "0.92324072887\n",
    "\n",
    "Elapsed time: 00:02:31\n",
    "\n",
    "XGBRegressor\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.2, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=1, missing=None, n_estimators=1800,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=1337,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "0.926241453432\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Regressors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.2, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=1, missing=None, n_estimators=1800,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=1337,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='ls', max_depth=2, max_features=100,\n",
    "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "             min_samples_leaf=1, min_samples_split=2,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "             presort='auto', random_state=1234, subsample=1.0, verbose=0,\n",
    "             warm_start=False)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
    "           max_features=100, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
    "           oob_score=True, random_state=458, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def cross_validation( regressor,features, target ):\n",
    "    stopwatch.start()\n",
    "    print(type(regressor).__name__)\n",
    "    y_pred = cross_val_predict(regressor, features, target, cv = 5, n_jobs = -1)\n",
    "    rmse, rmse_log, r2score = show_metrics( target, y_pred)\n",
    "    stopwatch.print_elapsed_time()\n",
    "    return rmse, rmse_log, r2score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor\n",
      "RMSE               : 21546.7702\n",
      "RMSE of logarithms : 0.1181\n",
      "R2 score           : 0.926484\n",
      "\n",
      "Elapsed time: 00:00:16\n",
      "\n",
      "GradientBoostingRegressor\n",
      "RMSE               : 22004.9835\n",
      "RMSE of logarithms : 0.1224\n",
      "R2 score           : 0.923324\n",
      "\n",
      "Elapsed time: 00:00:12\n",
      "\n",
      "RandomForestRegressor\n",
      "RMSE               : 25372.8165\n",
      "RMSE of logarithms : 0.1352\n",
      "R2 score           : 0.898058\n",
      "\n",
      "Elapsed time: 00:00:26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_rmse, xgb_rmse_log, xgb_r2 = cross_validation(xgb, features, target)\n",
    "gbr_rmse, gbr_rmse_log, gbr_r2 = cross_validation(gbr, features, target)\n",
    "rfr_rmse, rfr_rmse_log, rfr_r2 = cross_validation(rfr, features, target)\n",
    "\n",
    "sum_rmse = xgb_rmse + gbr_rmse + rfr_rmse\n",
    "sum_rmse_log = xgb_rmse_log + gbr_rmse_log + rfr_rmse_log\n",
    "sum_r2 = xgb_r2 + gbr_r2 + rfr_r2\n",
    "\n",
    "xgb_w = xgb_r2/sum_r2\n",
    "gbr_w = gbr_r2/sum_r2\n",
    "rfr_w = rfr_r2/sum_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train: (1458, 297)\n",
      "features_test: (1459, 297)\n",
      "\n",
      "Elapsed time: 00:00:08\n",
      "\n",
      "\n",
      "Elapsed time: 00:00:10\n",
      "\n",
      "\n",
      "Elapsed time: 00:00:15\n",
      "\n",
      "\n",
      "Elapsed time: 00:00:15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use all data for learning\n",
    "\n",
    "stopwatch.start()\n",
    "\n",
    "rfr.fit(features_train, target_train)\n",
    "print_shape( features_train, 'features_train')\n",
    "print_shape( features_test, 'features_test')\n",
    "\n",
    "predictions_rfr = rfr.predict(features_test)\n",
    "\n",
    "stopwatch.print_elapsed_time()\n",
    "gbr.fit(features_train, target_train)\n",
    "predictions_gbr = gbr.predict(features_test)\n",
    "stopwatch.print_elapsed_time()\n",
    "xgb.fit(features_train, target_train)\n",
    "predictions_xgb = xgb.predict(features_test)\n",
    "stopwatch.print_elapsed_time()\n",
    "\n",
    "\n",
    "#  stacked \n",
    "predictions = xgb_w* predictions_xgb + gbr_w * predictions_gbr + rfr_w * predictions_rfr\n",
    "#predictions = predictions_xgb\n",
    "\n",
    "out_preds = pd.DataFrame()\n",
    "out_preds['Id'] = test_ids\n",
    "out_preds['SalePrice'] = predictions\n",
    "out_preds.to_csv('output6.csv', index=False)\n",
    "\n",
    "stopwatch.print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
